{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30c1449-46ef-4e0f-acb4-0cfef43468a5",
   "metadata": {},
   "source": [
    "### Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807eb86-915e-40ab-aa8b-559595d6c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install sentencepiece\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7a44cf5-db4b-48cf-b4aa-5dbe363ce3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d906536f-5e92-469b-8b23-62eb1c809c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"slone/nllb-210-v1\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543cc2d-158a-4538-a124-6456c3ef1dab",
   "metadata": {},
   "source": [
    "### Скачивание модели с huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cc90111-c3f6-4e0a-bba3-2d9cebac3f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d80fc82ab934d93a5014c6842e7524c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d94438a0c8a4d4d89896c33ced3a921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3702ee6a6948b4ba39e34c3bb402ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94a609650184c02abdc82c880ddced5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81253ffd655c445284ab0e58c3340119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c803b35ad7e473882c8f4394ef2ed59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6b3ddf138643b8b7f3429975b9304b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/584 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17efddaaabbd40b0b4b103060839c132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5a24bb5b81464c938b48147c6e2701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flax_model.msgpack:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/olegslepcov/Projects/Studying/huggingface/models/DeepPavlov/rubert-base-cased'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(MODEL_NAME, local_dir=f\"./models/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12dc442-7e99-439f-b81a-9b44ab515b60",
   "metadata": {},
   "source": [
    "### Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6fd9732-7802-4087-ae1f-0633b5090b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6068fa1e89f94dd78b0237c4de098bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfc10db7d674e39a7ab3d125df52a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a507a4a77085435894b50a01b0059d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22203f3d6b1b45bcb6feee0f6e08a122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7540a6f5b37b48acaea70018caa10850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.55.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "model_path = f\"models/{MODEL_NAME}\"\n",
    "\n",
    "# Загрузка токенизатора используемого при создании модели\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Загрузка модели\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea53ff-413b-4d3d-b76d-2245dad07275",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f403344f-b76b-4f51-971f-b49642707094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes: torch.Size([3, 7])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = [\n",
    "    \"I love machine learning!\",\n",
    "    \"This is not good at all.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "# text = \"Договор между Ивановым Иваном Ивановичем и Петровым Петром Петровичем.\"\n",
    "\n",
    "\n",
    "# Токенизация\n",
    "inputs = tokenizer(\n",
    "    texts,\n",
    "    padding=True,        # Добиваем до одинаковой длины\n",
    "    truncation=True,     # Обрезаем слишком длинные тексты\n",
    "    max_length=512,      # Максимальная длина\n",
    "    return_tensors=\"pt\"  # Возвращаем тензоры PyTorch\n",
    ")\n",
    "\n",
    "print(\"Input shapes:\", inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e02999-8b1f-482c-99d2-55aebc3e7d4f",
   "metadata": {},
   "source": [
    "### Запуск модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2492bdaa-7e25-40ce-b2bc-043fdfe67f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([[3, 0, 6,  ..., 1, 1, 1],\n",
      "        [5, 5, 5,  ..., 0, 0, 0],\n",
      "        [4, 3, 3,  ..., 0, 0, 0]])\n",
      "Confidence scores: tensor([[[1.0731e-06, 3.5383e-05, 1.2774e-05,  ..., 5.4083e-12,\n",
      "          5.4083e-12, 5.4083e-12],\n",
      "         [3.0870e-05, 1.0055e-05, 1.3733e-07,  ..., 5.0468e-10,\n",
      "          5.0468e-10, 5.0469e-10],\n",
      "         [3.4299e-05, 6.4636e-07, 7.0810e-08,  ..., 2.5902e-11,\n",
      "          2.5902e-11, 2.5902e-11],\n",
      "         ...,\n",
      "         [6.3756e-06, 1.6727e-07, 4.5533e-05,  ..., 8.4607e-12,\n",
      "          8.4607e-12, 8.4607e-12],\n",
      "         [1.6162e-04, 1.9000e-07, 5.0877e-05,  ..., 4.3288e-12,\n",
      "          4.3288e-12, 4.3288e-12],\n",
      "         [6.0461e-04, 1.7242e-06, 3.5297e-04,  ..., 1.6939e-11,\n",
      "          1.6939e-11, 1.6939e-11]],\n",
      "\n",
      "        [[1.2136e-06, 1.9201e-07, 8.8113e-07,  ..., 2.3913e-10,\n",
      "          2.3913e-10, 2.3913e-10],\n",
      "         [6.0037e-07, 2.4078e-06, 1.1083e-07,  ..., 2.9706e-11,\n",
      "          2.9706e-11, 2.9706e-11],\n",
      "         [1.0572e-05, 8.8316e-06, 1.0251e-07,  ..., 4.9815e-11,\n",
      "          4.9815e-11, 4.9815e-11],\n",
      "         ...,\n",
      "         [7.3798e-06, 4.4909e-06, 8.9572e-08,  ..., 3.5504e-11,\n",
      "          3.5504e-11, 3.5504e-11],\n",
      "         [6.0825e-02, 7.7087e-04, 3.4523e-06,  ..., 1.4736e-11,\n",
      "          1.4736e-11, 1.4736e-11],\n",
      "         [3.3478e-07, 1.4811e-09, 6.4068e-08,  ..., 6.1840e-11,\n",
      "          6.1840e-11, 6.1840e-11]],\n",
      "\n",
      "        [[2.7978e-08, 1.4489e-07, 1.2977e-07,  ..., 2.8167e-10,\n",
      "          2.8167e-10, 2.8167e-10],\n",
      "         [1.0543e-04, 5.7542e-05, 2.9369e-07,  ..., 1.2422e-11,\n",
      "          1.2422e-11, 1.2422e-11],\n",
      "         [4.1546e-05, 5.9168e-05, 2.7051e-07,  ..., 4.1307e-11,\n",
      "          4.1307e-11, 4.1308e-11],\n",
      "         ...,\n",
      "         [7.1331e-03, 6.7570e-05, 3.4610e-07,  ..., 1.4035e-12,\n",
      "          1.4035e-12, 1.4035e-12],\n",
      "         [5.7040e-08, 1.1664e-10, 1.0469e-09,  ..., 1.7616e-11,\n",
      "          1.7616e-11, 1.7616e-11],\n",
      "         [8.6573e-06, 1.1683e-07, 1.4248e-07,  ..., 6.9548e-12,\n",
      "          6.9548e-12, 6.9548e-12]]])\n"
     ]
    }
   ],
   "source": [
    "# Переводим модель в режим оценки\n",
    "model.eval()\n",
    "\n",
    "# Переносим на GPU если доступно\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Получаем предсказания\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Интерпретируем результаты\n",
    "predicted_classes = torch.argmax(predictions, dim=1)\n",
    "print(\"Predictions:\", predicted_classes)\n",
    "print(\"Confidence scores:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26544b-e1df-410d-ab83-05c0d70c26b7",
   "metadata": {},
   "source": [
    "### Пост-обработка результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "085cbb57-99fe-4563-88c4-cb072465ef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love machine learning!\n",
      "Prediction: tensor([3, 0, 6,  ..., 1, 1, 1])\n",
      "Confidence: 0.8881\n",
      "--------------------------------------------------\n",
      "Text: This is not good at all.\n",
      "Prediction: tensor([5, 5, 5,  ..., 0, 0, 0])\n",
      "Confidence: 0.9505\n",
      "--------------------------------------------------\n",
      "Text: The weather is nice today.\n",
      "Prediction: tensor([4, 3, 3,  ..., 0, 0, 0])\n",
      "Confidence: 0.6654\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# # Если у модели есть метки классов\n",
    "# if hasattr(model.config, 'id2label'):\n",
    "#     labels = [model.config.id2label[idx] for idx in predicted_classes.tolist()]\n",
    "#     print(\"Predicted labels:\", labels)\n",
    "# else:\n",
    "#     print(\"Raw predictions:\", predicted_classes.tolist())\n",
    "\n",
    "# Для каждой строки выводим результат\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {labels[i] if 'labels' in locals() else predicted_classes[i]}\")\n",
    "    print(f\"Confidence: {predictions[i].max().item():.4f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151635ce-9116-46e2-b246-f6c22a9b89c1",
   "metadata": {},
   "source": [
    "### Настройка обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d1044-fffd-4fe5-b6da-87e8a758285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Аргументы обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",  # Директория для сохранения\n",
    "    num_train_epochs=3,              # Количество эпох\n",
    "    per_device_train_batch_size=8,   # Размер батча\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,              # Скорость обучения\n",
    "    weight_decay=0.01,               # Вес decay\n",
    "    logging_dir='./logs',            # Директория для логов\n",
    "    evaluation_strategy=\"epoch\",     # Стратегия оценки\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03c7a4-8eed-4be8-84eb-560958143001",
   "metadata": {},
   "source": [
    "### Создание и запуск тренера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486db552-49a9-4db2-95a4-ecd0de860fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Data collator для автоматического padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Создаем тренер\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  # В реальности нужен отдельный eval set\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Запускаем обучение\n",
    "trainer.train()\n",
    "\n",
    "# Сохраняем дообученную модель\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d06548-6fe2-4651-9cda-80939b962cc6",
   "metadata": {},
   "source": [
    "### Тестирование дообученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a8a46-6033-4da5-b891-c1d2b78bafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем дообученную модель\n",
    "fine_tuned_model = AutoModelForSequenceClassification.from_pretrained(\"./fine_tuned_model\")\n",
    "fine_tuned_model.to(device)\n",
    "\n",
    "# Тестируем на новых данных\n",
    "test_text = \"This product exceeded my expectations!\"\n",
    "test_input = tokenizer(test_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = fine_tuned_model(**test_input)\n",
    "    prediction = torch.softmax(output.logits, dim=-1)\n",
    "    \n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Prediction: {prediction.argmax().item()}\")\n",
    "print(f\"Confidence: {prediction.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d12363-0a36-4a59-9fa5-214adfe7159a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ec08c-4357-4608-a1df-2a1c60528e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef38b2-bb60-406d-842b-71132ad23186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7fc691a-61d0-4e05-b63b-bb764c3e2a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33686e15ab5e4a49a3e5497cdbafd801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extract the key points from the following article:\n",
      "\n",
      "\n",
      "[Your article text here]\n",
      "\n",
      "Here are the key points from the article:\n",
      "\n",
      "1. **Introduction to the Problem**: The article discusses the challenges faced by individuals with disabilities in accessing public transportation, specifically focusing on the need for accessible and inclusive systems.\n",
      "\n",
      "2. **Current State of Accessibility**: It highlights the current state of public transportation accessibility, noting that while some improvements have been made, there is still a significant gap in meeting the needs of people with disabilities.\n",
      "\n",
      "3. **Barriers to Access**: The article identifies several barriers that prevent people with disabilities from using public transportation effectively, including physical barriers, lack of information, and inadequate support services.\n",
      "\n",
      "4. **Case Studies**: It presents case studies of individuals who have faced difficulties due to these barriers, emphasizing the personal impact on their lives and well-being.\n",
      "\n",
      "5. **Recommendations for Improvement**: The article suggests several recommendations for improving accessibility, including the need for more inclusive design, better information provision, and enhanced support services.\n",
      "\n",
      "6. **Conclusion**: The article concludes by reiterating the importance of addressing these issues and calls for collective action to ensure that public transportation is truly accessible to all members of society.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "article = \"\"\"\n",
    "[Your article text here]\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Extract the key points from the following article:\n",
    "\n",
    "{article}\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=1024)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3bf75-76b4-4944-b91b-06834ac79c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "article = \"\"\"\n",
    "[Your article text here]\n",
    "\"\"\"\n",
    "\n",
    "text = \"Договор между Ивановым Иваном Ивановичем и Петровым Петром Петровичем.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=1024)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80897fba-dcbd-45ad-bed7-9a66405bf21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a0df0-33a0-45e0-b7aa-014a2ab31af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
